# Deployment Configuration for Mac Mini Optimization

# Target hardware specifications
hardware:
  device_type: "mac_mini"
  memory_gb: 16
  cpu_cores: 8
  gpu_type: "apple_silicon"  # "apple_silicon", "intel", "none"
  use_neural_engine: true

# Quantization settings for deployment
quantization:
  # Main quantization configuration
  enable: true
  method: "gptq"  # "gptq", "awq", "bnb", "dynamic"
  
  # Bit precision
  bits: 4
  group_size: 32
  desc_act: false
  static_groups: false
  
  # Calibration for quantization
  calibration:
    dataset: "wikitext2"
    num_samples: 512
    seq_len: 512
    use_fast: true
  
  # Post-quantization optimization
  optimize_for_inference: true
  fuse_layers: true

# Model optimization for Apple Silicon
apple_optimization:
  # CoreML conversion
  coreml:
    enable: true
    compute_units: "cpuAndNeuralEngine"  # "cpuOnly", "cpuAndGPU", "cpuAndNeuralEngine"
    minimum_deployment_target: "iOS15"
    optimize_for: "speed"  # "speed", "size"
    quantize_weights: true
    precision: "float16"
  
  # BNNS optimization
  bnns:
    enable: true
    use_bnns_graph: true
  
  # Metal Performance Shaders
  mps:
    enable: true
    use_mps_graph: true

# Inference optimization
inference:
  # Memory management
  max_memory_fraction: 0.8
  memory_efficient: true
  use_cache: true
  cache_implementation: "static"  # "static", "dynamic"
  
  # Batch processing
  max_batch_size: 1
  dynamic_batching: false
  
  # Generation parameters
  max_new_tokens: 512
  do_sample: true
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  
  # Performance optimization
  use_compiled_model: true
  torch_compile: true
  compile_mode: "default"  # "default", "reduce-overhead", "max-autotune"

# Deployment formats
formats:
  # GGUF format for llama.cpp
  gguf:
    enable: true
    quantization_type: "Q4_K_M"  # Q4_0, Q4_1, Q4_K_S, Q4_K_M, Q8_0
    vocab_type: "GGML"
    
  # ONNX format
  onnx:
    enable: false
    opset_version: 17
    dynamic_axes: true
    optimize: true
    
  # TensorRT (for NVIDIA GPUs)
  tensorrt:
    enable: false
    precision: "fp16"
    max_workspace_size: 1073741824  # 1GB
    
  # OpenVINO (for Intel)
  openvino:
    enable: false
    precision: "FP16"
    optimization_level: "PERFORMANCE"

# Framework-specific configurations
frameworks:
  # llama.cpp configuration
  llamacpp:
    enable: true
    threads: 8
    use_metal: true
    use_gpu: true
    gpu_layers: 32
    context_size: 2048
    batch_size: 512
    
  # Ollama configuration
  ollama:
    enable: true
    modelfile_template: |
      FROM ./model.gguf
      PARAMETER temperature 0.7
      PARAMETER top_p 0.9
      PARAMETER top_k 50
      PARAMETER repeat_penalty 1.1
      PARAMETER num_ctx 2048
      PARAMETER num_predict 512
      SYSTEM "You are a helpful AI assistant."
    
  # Transformers optimizations
  transformers:
    use_bettertransformer: true
    torch_dynamo: true
    use_flash_attention: false  # Not available on Mac
    
# Memory optimization strategies
memory:
  # Memory mapping
  use_memory_mapping: true
  mmap_weights: true
  
  # CPU offloading
  cpu_offload: false
  
  # Memory pools
  use_memory_pool: true
  pool_size_mb: 2048
  
  # Garbage collection
  aggressive_gc: true
  gc_threshold: 0.1

# Performance monitoring
monitoring:
  # Enable performance tracking
  enable: true
  
  # Metrics to track
  metrics:
    - "inference_time"
    - "memory_usage"
    - "cpu_usage"
    - "gpu_usage"
    - "tokens_per_second"
    - "latency_p95"
    - "latency_p99"
  
  # Logging configuration
  log_level: "INFO"
  log_file: "./logs/deployment.log"
  
  # Benchmarking
  benchmark:
    enable: true
    num_runs: 100
    warmup_runs: 10
    input_lengths: [10, 50, 100, 200, 500]

# API server configuration
server:
  # Basic server settings
  host: "localhost"
  port: 8000
  workers: 1
  
  # API configuration
  api_version: "v1"
  max_request_size: 1048576  # 1MB
  request_timeout: 300
  
  # Rate limiting
  rate_limit:
    enable: true
    requests_per_minute: 60
    burst_size: 10
  
  # CORS settings
  cors:
    enable: true
    allow_origins: ["*"]
    allow_methods: ["GET", "POST"]
    allow_headers: ["*"]

# Deployment targets
targets:
  # Local deployment
  local:
    enable: true
    install_dependencies: true
    create_service: true
    
  # Docker deployment
  docker:
    enable: false
    base_image: "python:3.9-slim"
    requirements_file: "requirements.txt"
    
  # Package distribution
  package:
    enable: false
    format: "wheel"
    include_model: true
    compress: true

# Testing and validation
testing:
  # Correctness testing
  verify_outputs: true
  reference_model: "original"
  tolerance: 0.01
  
  # Performance testing
  benchmark_against_reference: true
  acceptable_slowdown: 1.5  # 50% slower is acceptable
  
  # Memory testing
  test_memory_usage: true
  max_memory_increase: 1.2  # 20% more memory is acceptable
  
  # Stress testing
  stress_test: true
  max_concurrent_requests: 10
  stress_duration_minutes: 10

# Fallback configurations
fallback:
  # If quantization fails
  disable_quantization: true
  
  # If Apple optimizations fail
  use_cpu_only: true
  
  # If compilation fails
  use_eager_mode: true
  
  # Memory fallbacks
  reduce_batch_size: true
  enable_cpu_offload: true