# Core PyTorch and ML dependencies
torch>=2.0.0
torchvision
torchaudio

# Transformers and NLP
transformers>=4.30.0
tokenizers>=0.13.0
datasets>=2.10.0
evaluate>=0.4.0

# Training and optimization
accelerate>=0.20.0
deepspeed>=0.9.0  # Optional, for advanced training
bitsandbytes>=0.39.0  # Optional, for quantization alternatives

# Data and utilities
numpy>=1.21.0
scipy>=1.7.0
pandas>=1.3.0
matplotlib>=3.5.0
seaborn>=0.11.0
Pillow>=8.3.0

# Configuration and logging
PyYAML>=6.0
omegaconf>=2.3.0  # Alternative config system
wandb>=0.15.0  # Weights & Biases for experiment tracking
tensorboard>=2.10.0
tqdm>=4.64.0

# Development and testing
pytest>=7.0.0
pytest-cov>=4.0.0
black>=22.0.0  # Code formatting
flake8>=5.0.0  # Linting
mypy>=0.991  # Type checking

# Quantization and optimization
safetensors>=0.3.0  # Safe tensor serialization
sentencepiece>=0.1.97  # For tokenization
protobuf>=3.20.0

# Optional: Apple-specific optimizations
# These are automatically available on macOS with Apple Silicon
# coremltools>=6.0  # For Core ML export
# ane_transformers  # Apple Neural Engine optimizations

# Optional: Advanced features
# flash-attn>=2.0.0  # Flash attention (limited Mac support)
# xformers>=0.0.20  # Memory-efficient transformers
# triton>=2.0.0  # Triton kernels (limited Mac support)

# Utilities
requests>=2.28.0
aiohttp>=3.8.0
psutil>=5.9.0  # System monitoring
GPUtil>=1.4.0  # GPU monitoring (limited use on Mac)
py3nvml>=0.2.7  # NVML for GPU monitoring (NVIDIA only)

# Jupyter and interactive development
jupyter>=1.0.0
jupyterlab>=3.4.0
ipywidgets>=8.0.0
notebook>=6.4.0

# Data processing
jsonlines>=3.1.0
h5py>=3.7.0
zarr>=2.12.0  # For large dataset storage

# Visualization
plotly>=5.10.0
bokeh>=2.4.0
altair>=4.2.0

# API and deployment
fastapi>=0.85.0
uvicorn>=0.18.0
gradio>=3.40.0  # For web UI
streamlit>=1.25.0  # Alternative web UI

# Model export and deployment
onnx>=1.12.0
onnxruntime>=1.12.0
coremltools>=6.3.0  # Apple Core ML
optimum>=1.12.0  # Hugging Face model optimization

# Development dependencies
pre-commit>=2.20.0
sphinx>=5.0.0  # Documentation
sphinx-rtd-theme>=1.0.0
nbsphinx>=0.8.9  # Jupyter notebook documentation

# Optional: Knowledge distillation
torch-distillation>=0.1.0  # If available
knowledge-distillation>=1.0.0  # Alternative

# Optional: Advanced training techniques
pytorch-lightning>=1.7.0  # Alternative training framework
hydra-core>=1.2.0  # Advanced configuration management
mlflow>=1.28.0  # ML experiment tracking

# Memory and performance profiling
memory-profiler>=0.60.0
line-profiler>=3.5.1
py-spy>=0.3.12  # Python profiler
scalene>=1.5.13  # Advanced profiler

# Distributed computing (optional)
ray>=2.0.0  # Distributed computing
horovod>=0.28.0  # Distributed training (limited Mac support)

# Model compression and quantization
neural-compressor>=2.0  # Intel neural compressor
pruning>=1.0.0  # Model pruning utilities

# Benchmarking and evaluation
bert-score>=0.3.13
rouge-score>=0.1.2
sacrebleu>=2.3.1
nltk>=3.8
spacy>=3.4.0

# Time series and scientific computing
scikit-learn>=1.1.0
statsmodels>=0.13.0
sympy>=1.11.0

# Async and parallel processing
asyncio-throttle>=1.0.2
concurrent-futures>=3.1.1
multiprocess>=0.70.14