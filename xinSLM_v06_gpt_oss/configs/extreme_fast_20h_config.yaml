# EXTREME Fast 20-Hour Training Configuration
# Ultra-aggressive optimizations for 20h completion

training:
  # Model configuration - absolute minimal
  model_name: "gpt-oss-moe-extreme-fast-20h"
  model_variant: "light"  # Light instead of nano for stability
  
  # Multi-dataset configuration - minimal data
  use_multi_datasets: true
  train_datasets:
    - name: "wikitext-103"
      max_samples: 5000     # Drastically reduced
      weight: 0.6           # 60% WikiText-103
      
    - name: "wikitext-2" 
      max_samples: 2000     # Very small
      weight: 0.4           # 40% WikiText-2
  
  eval_datasets:
    - name: "wikitext-2"
      max_samples: 500      # Minimal eval set
      weight: 1.0
  
  # Data configuration - ultra minimal
  tokenizer_name: "gpt2"
  max_seq_length: 64      # Extremely short sequences
  
  # Batch configuration - optimized for speed
  per_device_train_batch_size: 16   # Large batches for efficiency  
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 1    # No accumulation
  dataloader_num_workers: 0
  
  # Training loop - extreme speed focus
  num_train_epochs: 5     # More epochs on tiny data
  max_steps: 1000         # Very low step count for 20h
  overwrite_output_dir: true
  
  # Optimizer - aggressive
  optimizer: "adamw"
  learning_rate: 1e-3     # Very high LR
  weight_decay: 0.0       # No weight decay for speed
  beta1: 0.9
  beta2: 0.95
  epsilon: 1e-8
  max_grad_norm: 2.0      # Higher threshold
  
  # Learning rate schedule
  lr_scheduler_type: "linear"
  warmup_steps: 50        # Minimal warmup
  
  # Mixed precision - essential
  fp16: true
  bf16: false
  fp16_opt_level: "O2"    # More aggressive
  
  # Memory optimization - prioritize speed
  gradient_checkpointing: false
  dataloader_pin_memory: false
  remove_unused_columns: true
  
  # MoE specific - minimal
  moe_aux_loss_coef: 0.0      # Disabled for speed
  moe_z_loss_coef: 0.0        # Disabled for speed  
  router_aux_loss_coef: 0.0   # Disabled for speed
  
  # Logging - minimal overhead
  logging_steps: 25
  save_steps: 200           # Frequent saves for monitoring
  eval_steps: 100           # Frequent eval
  save_total_limit: 1       # Only 1 checkpoint
  evaluation_strategy: "steps"
  
  # Output
  output_dir: "./checkpoints_extreme_fast_20h"
  logging_dir: "./logs_extreme_fast_20h"
  
  load_best_model_at_end: false

# Mac optimizations - extreme
mac_optimizations:
  device: "mps"
  cpu_fallback: true
  
  # Memory - conservative for stability
  empty_cache_interval: 25       # Very frequent cleanup
  max_memory_allocation: "8GB"   # Much lower limit
  
  # Threading
  num_workers: 0
  persistent_workers: false
  prefetch_factor: 1
  
  use_metal_performance_shaders: true
  optimize_for_inference: false

# Data preprocessing - ultra minimal
data:
  tokenizer_path: "gpt2"
  add_special_tokens: true
  padding_side: "right"
  truncation: true
  
  min_seq_length: 8       # Very short
  max_seq_length: 64      # Very short
  filter_empty_examples: true

# Checkpointing - absolute minimal
checkpointing:
  resume_from_checkpoint: null
  
  save_optimizer_states: false
  save_scheduler_states: false  
  save_random_states: false
  
  save_best_model: false
  save_last_model: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

# Evaluation - bare minimum
evaluation:
  eval_datasets: ["wikitext-2"]
  metrics: ["loss"]
  
  eval_during_training: true
  eval_at_start: false
  eval_accumulation_steps: null
  
  eval_generation: false

# Monitoring - disabled for speed
monitoring:
  use_wandb: false
  use_tensorboard: false
  
  log_level: "ERROR"              # Minimal logging
  log_predictions: false
  log_model_architecture: false
  
  disable_tqdm: false
  tqdm_update_interval: 25

# Resource monitoring - critical only
resource_monitoring:
  monitor_cpu: false
  monitor_memory: true             # Keep memory only
  monitor_gpu: false
  monitor_disk: false
  
  memory_threshold: 0.85           # Lower threshold
  temperature_threshold: 80
  
  monitoring_interval: 120         # Very infrequent
  save_monitoring_data: false

# Model architecture - ultra-light
model:
  vocab_size: 50257
  n_positions: 64                  # Very short context
  n_embd: 192                      # Very small embedding
  n_layer: 4                       # Very few layers
  n_head: 6                        # Few heads
  n_inner: 384                     # Small MLP
  
  # MoE - minimal
  num_experts: 2                   # Minimal experts
  top_k_experts: 1                 # Use only 1 expert
  expert_capacity_factor: 1.0
  
  # No regularization for speed
  dropout: 0.0
  attention_dropout: 0.0
  activation_dropout: 0.0
  
  initializer_range: 0.02
  layer_norm_epsilon: 1e-5