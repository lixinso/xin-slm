# Multi-WikiText Dataset Training Configuration for GPT-OSS MoE Model
# Uses WikiText-103 + WikiText-2 for reliable multi-dataset training

training:
  # Model configuration
  model_name: "gpt-oss-moe-wikitext-multi"
  model_variant: "light"  # Use light variant for stable training
  
  # Multi-dataset configuration
  use_multi_datasets: true
  train_datasets:
    - name: "wikitext-103"
      max_samples: 60000    # Larger WikiText for main training
      weight: 0.8           # 80% WikiText-103
      
    - name: "wikitext-2"
      max_samples: null     # Use full WikiText-2
      weight: 0.2           # 20% WikiText-2
  
  eval_datasets:
    - name: "wikitext-2"
      max_samples: null
      weight: 1.0
  
  # Data configuration
  tokenizer_name: "gpt2"
  max_seq_length: 512     
  
  # Batch configuration - optimized for multi-dataset
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 32  # Effective batch size = 32
  dataloader_num_workers: 2
  
  # Training loop
  num_train_epochs: 2     
  max_steps: -1
  
  # Optimizer configuration
  optimizer: "adamw"
  learning_rate: 3e-4     # Standard learning rate
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  epsilon: 1e-8
  max_grad_norm: 1.0
  
  # Learning rate schedule
  lr_scheduler_type: "cosine"
  warmup_steps: 1000      
  warmup_ratio: 0.05
  
  # Mixed precision training
  fp16: true
  bf16: false
  fp16_opt_level: "O1"
  
  # Memory optimization
  gradient_checkpointing: true
  dataloader_pin_memory: false
  remove_unused_columns: true
  
  # MoE specific training
  moe_aux_loss_coef: 0.02
  moe_z_loss_coef: 0.001
  router_aux_loss_coef: 0.02
  
  # Logging and saving
  logging_steps: 25
  save_steps: 500         
  eval_steps: 250         
  save_total_limit: 3     
  evaluation_strategy: "steps"
  
  # Output directories
  output_dir: "./checkpoints_wikitext_multi"
  logging_dir: "./logs_wikitext_multi"
  
  # Early stopping
  early_stopping_patience: 5
  early_stopping_threshold: 0.01

# Mac Mini specific optimizations
mac_optimizations:
  # Device configuration
  device: "mps"
  cpu_fallback: true
  
  # Memory management
  empty_cache_interval: 50   
  max_memory_allocation: "10GB"  # Conservative for reliability
  
  # Threading
  num_workers: 2
  persistent_workers: true
  prefetch_factor: 2         
  
  # Apple-specific optimizations
  use_metal_performance_shaders: true
  optimize_for_inference: false

# Data preprocessing
data:
  tokenizer_path: "gpt2"
  add_special_tokens: true
  padding_side: "right"
  truncation: true
  
  # Data filtering 
  min_seq_length: 32     
  max_seq_length: 512     
  filter_empty_examples: true

# Checkpointing and resuming
checkpointing:
  resume_from_checkpoint: null
  
  save_optimizer_states: true
  save_scheduler_states: true
  save_random_states: true
  
  save_best_model: true
  save_last_model: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

# Evaluation configuration
evaluation:
  eval_datasets: ["wikitext-2"]
  
  metrics: ["perplexity", "loss"]
  
  eval_during_training: true
  eval_at_start: true
  eval_accumulation_steps: null
  
  # Generation evaluation
  eval_generation: false  # Disabled for faster training
  generation_max_length: 100  
  generation_num_samples: 2

# Monitoring and logging
monitoring:
  use_wandb: false
  wandb_project: "xinslm-v06-wikitext-multi"
  
  use_tensorboard: true
  tensorboard_log_dir: "./logs_wikitext_multi/tensorboard"
  
  log_level: "INFO"
  log_predictions: false      
  log_model_architecture: true
  
  disable_tqdm: false
  tqdm_update_interval: 1

# Resource monitoring
resource_monitoring:
  monitor_cpu: true
  monitor_memory: true
  monitor_gpu: true
  monitor_disk: true
  
  # Safe thresholds
  memory_threshold: 0.85     # Conservative threshold
  temperature_threshold: 80  
  
  monitoring_interval: 30    
  save_monitoring_data: true
  monitoring_output_file: "./logs_wikitext_multi/resource_usage.json"